{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir breast_cancer_analysis\n",
    "cd breast_cancer_analysis\n",
    "git init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m venv venv\n",
    "source venv/bin/activate  # For MacOS/Linux\n",
    "venv\\Scripts\\activate     # For Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Acquisition and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The dataset does not contain a 'target' column.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Check if the dataset has a target column\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset does not contain a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The dataset does not contain a 'target' column."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('data 2.csv')\n",
    "\n",
    "# Check if the dataset has a target column\n",
    "if 'target' not in df.columns:\n",
    "    raise ValueError(\"The dataset does not contain a 'target' column.\")\n",
    "\n",
    "# Preprocess the data\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save preprocessed data\n",
    "pd.DataFrame(X_train).to_csv('X_train.csv', index=False)\n",
    "pd.DataFrame(X_test).to_csv('X_test.csv', index=False)\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv', index=False)\n",
    "pd.DataFrame(y_test).to_csv('y_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "\n",
    "# Select the top k features\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Save the selected features\n",
    "pd.DataFrame(X_train_selected).to_csv('X_train_selected.csv', index=False)\n",
    "pd.DataFrame(selector.transform(pd.read_csv('X_test.csv'))).to_csv('X_test_selected.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search CV for Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load selected features\n",
    "X_train_selected = pd.read_csv('X_train_selected.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50, 50), (50, 100, 50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "# Initialize the MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Save the best parameters\n",
    "with open('best_params.txt', 'w') as f:\n",
    "    f.write(str(best_params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing an Artificial Neural Network (ANN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load selected features\n",
    "X_train_selected = pd.read_csv('X_train_selected.csv')\n",
    "y_train = pd.read_csv('y_train.csv').values.ravel()\n",
    "X_test_selected = pd.read_csv('X_test_selected.csv')\n",
    "y_test = pd.read_csv('y_test.csv').values.ravel()\n",
    "\n",
    "# Load the best parameters\n",
    "with open('best_params.txt', 'r') as f:\n",
    "    best_params = eval(f.read())\n",
    "\n",
    "# Initialize the MLPClassifier with the best parameters\n",
    "best_mlp = MLPClassifier(**best_params)\n",
    "\n",
    "# Train the model\n",
    "best_mlp.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_mlp.predict(X_test_selected)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the trained model\n",
    "import joblib\n",
    "joblib.dump(best_mlp, 'best_mlp_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Streamlit App Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Load data and model\n",
    "df = pd.read_csv('/mnt/data/data 2.csv')\n",
    "model = joblib.load('best_mlp_model.pkl')\n",
    "\n",
    "# Preprocess data for display\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "st.title(\"Breast Cancer Prediction App\")\n",
    "st.write(\"Interact with the dataset and get predictions\")\n",
    "\n",
    "# User input for prediction\n",
    "user_input = []\n",
    "for i, col in enumerate(df.columns[:-1]):\n",
    "    user_input.append(st.slider(col, float(X_selected[:, i].min()), float(X_selected[:, i].max()), float(X_selected[:, i].mean())))\n",
    "\n",
    "# Predict\n",
    "if st.button(\"Predict\"):\n",
    "    prediction = model.predict([user_input])\n",
    "    st.write(f\"The predicted class is: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment and Version Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git add .\n",
    "git commit -m \"Initial commit\"\n",
    "git remote add origin <your-repository-url>\n",
    "git push -u origin master\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
